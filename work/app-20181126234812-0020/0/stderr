18/11/26 23:48:14 ERROR Executor: Exception in task 2.0 in stage 5.0 (TID 17)
java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.AbstractTraversable.reduceLeft(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at edu.hku.cs.dp.dpobject$$anonfun$3.apply(dpobject.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:263)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/26 23:48:14 ERROR Executor: Exception in task 2.1 in stage 5.0 (TID 18)
java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.AbstractTraversable.reduceLeft(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at edu.hku.cs.dp.dpobject$$anonfun$3.apply(dpobject.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:263)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/26 23:48:14 ERROR Executor: Exception in task 2.2 in stage 5.0 (TID 19)
java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.AbstractTraversable.reduceLeft(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at edu.hku.cs.dp.dpobject$$anonfun$3.apply(dpobject.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:263)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/26 23:48:14 ERROR Executor: Exception in task 2.3 in stage 5.0 (TID 20)
java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.AbstractTraversable.reduceLeft(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at edu.hku.cs.dp.dpobject$$anonfun$3.apply(dpobject.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:263)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/26 23:48:14 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
0)
java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.AbstractTraversable.reduceLeft(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at edu.hku.cs.dp.dpobject$$anonfun$3.apply(dpobject.scala:58)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:263)
	at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
